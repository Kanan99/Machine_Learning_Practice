{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bag-of-words with Classification Models.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrY5UEKoeHeq",
        "outputId": "157f384a-2081-4fa2-a7bb-96fc7bdd0f11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#@title Importing the libraries and loading the data\n",
        "# Main libraries\n",
        "import pandas as pd\n",
        "pd.set_option(\"display.precision\", 4)\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# Text processing libraries\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Machine Learning libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Importing the dataset\n",
        "df = pd.read_csv('reviews.tsv', delimiter='\\t',\n",
        "                 quoting = 3) # 3 is for ignoring \"\" (double quotes)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title NLP object\n",
        "class NLP(object):\n",
        "  def __init__(self, data: pd.DataFrame):\n",
        "    self.df = data\n",
        "    self.y = self.df.iloc[:, -1].values\n",
        "  \n",
        "  def create_stopwords(self) -> list:\n",
        "    \"\"\"\n",
        "    Creates English stopwords from nltk package,\n",
        "    Excludes negative words for further sentiment analysis\n",
        "    Returns a list of stopwords\n",
        "    \"\"\"\n",
        "    self.all_stopwords = stopwords.words('english')\n",
        "    negative_words = ['not', \"don't\", \"aren't\", \"didn't\", \"hadn't\", \"hasn't\", \n",
        "                  \"haven't\", \"wasn't\", \"weren't\"]\n",
        "    for word in negative_words:\n",
        "      self.all_stopwords.remove(word)\n",
        "    return self.all_stopwords\n",
        "\n",
        "  def process_review(self, index: int) -> list:\n",
        "    \"\"\"\n",
        "    Takes the index to the DataFrame,\n",
        "    first removes punctuations,\n",
        "    and second lowers the letters.\n",
        "    Returns a list including strings.\n",
        "    \"\"\"\n",
        "    review = re.sub('[^a-zA-Z]', ' ', self.df.iloc[:, 0][index])\n",
        "    review = review.lower()\n",
        "    return review.split()\n",
        "\n",
        "  def remove_special_words(self, reviews: list) -> list:\n",
        "    \"\"\"\n",
        "    Takes the stemmed or lemmatized reviews,\n",
        "    Identifies the words used once as a special word,\n",
        "    Returns the reviews without the special words.\n",
        "    \"\"\"\n",
        "    words = [word for s in reviews for word in s.split()]\n",
        "    counted = Counter(words)\n",
        "    special_words = [word for word in counted.keys() if counted[word] == 1]\n",
        "    final_reviews = reviews[:]\n",
        "    for i, sentence in enumerate(final_reviews):\n",
        "      new_sentence = []\n",
        "      for word in sentence.split():\n",
        "        if word not in special_words:\n",
        "          new_sentence.append(word)\n",
        "      final_reviews[i] = ' '.join(new_sentence)\n",
        "    return final_reviews\n",
        "\n",
        "  def apply_stemmer(self) -> list:\n",
        "    \"\"\"\n",
        "    Fetches the reviews from the DataFrame,\n",
        "    (I) process the individual review via process_review(),\n",
        "    (II) removes the stopwords taken from create_stopwords(),\n",
        "    (III) removes the conjuctions of verbs (e.g. loved -> love) via Stemmer,\n",
        "    (IV) removes the special_words via remove_special_words().\n",
        "    Returns a list for Bag-of-words model\n",
        "    \"\"\"\n",
        "    corpus, stop_words = [], self.create_stopwords()\n",
        "    for i in range(self.df.shape[0]):\n",
        "      # Step I\n",
        "      review = self.process_review(i)\n",
        "      # Step II and III\n",
        "      stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
        "      review = [stemmer.stem(word) for word in review\n",
        "              if not word in set(stop_words)]\n",
        "      corpus.append(' '.join(review))\n",
        "\n",
        "    # Step IV\n",
        "    review = self.remove_special_words(corpus)\n",
        "    return corpus\n",
        "  \n",
        "  def apply_lemmatizer(self) -> list:\n",
        "    \"\"\"\n",
        "    Fetches the reviews from the DataFrame,\n",
        "    (I) process the individual review via process_review(),\n",
        "    (II) removes the stopwords taken from create_stopwords(),\n",
        "    (III) removes the conjuctions of verbs (e.g. loved -> love) via Lemmatizer,\n",
        "    (IV) removes the special_words via remove_special_words().\n",
        "    Returns a list for Bag-of-words model\n",
        "    \"\"\"\n",
        "    corpus, stop_words = [], self.create_stopwords()\n",
        "    for i in range(self.df.shape[0]):\n",
        "      # Step I\n",
        "      review = self.process_review(i)\n",
        "      # Step II and III\n",
        "      lemmatizer = WordNetLemmatizer()\n",
        "      review = [lemmatizer.lemmatize(word, pos = 'v') for word in review\n",
        "              if not word in set(stop_words)]\n",
        "      corpus.append(' '.join(review))\n",
        "    \n",
        "    # Step IV\n",
        "    review = self.remove_special_words(corpus)\n",
        "    return corpus\n",
        "\n",
        "  def bow(self) -> list:\n",
        "    \"\"\"\n",
        "    Fetches the data and applies first stemmer and then lemmatizer\n",
        "    to build the bag-of-word vector.\n",
        "    Returns a list as [X_stemmer, X_lemmatizer]\n",
        "    \"\"\"\n",
        "    reviews_stemmer = self.apply_stemmer()\n",
        "    reviews_lemmatizer = self.apply_lemmatizer()\n",
        "    dataset = []\n",
        "    for r in [reviews_stemmer, reviews_lemmatizer]:\n",
        "      cv = CountVectorizer()\n",
        "      dataset.append(cv.fit_transform(r).toarray())\n",
        "    return dataset"
      ],
      "metadata": {
        "cellView": "form",
        "id": "XH497sNxfDHt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Training the Classification models on Bag-of-words \n",
        "\n",
        "def list_classifiers():\n",
        "    classifiers = {\n",
        "        'Logistic Regression' : LogisticRegression(random_state = 42),\n",
        "        'K-Nearest Neighbors' : KNeighborsClassifier(n_neighbors = 5, p = 2, \n",
        "                                                     metric = 'minkowski'),\n",
        "        'Support Vector Machine' : SVC(kernel = 'linear', random_state = 42),\n",
        "        'Kernel SVM' : SVC(kernel = 'rbf', random_state = 42),\n",
        "        'Naive Bayes' : GaussianNB(),\n",
        "        'Decision Tree' : DecisionTreeClassifier(criterion = 'entropy', \n",
        "                                                 random_state = 42),\n",
        "        'Random Forest' : RandomForestClassifier(n_estimators=100, \n",
        "                                                 criterion='entropy',\n",
        "                                                 random_state = 42),\n",
        "    }\n",
        "    return classifiers\n",
        "\n",
        "# Get the classifiers\n",
        "classifiers = list_classifiers()\n",
        "# Define NLP() object with data and build Bag-of-words vector with bow()\n",
        "model = NLP(df)\n",
        "data = model.bow() \n",
        "# {classifier : [stemmer_score, lemmatizer_score]}\n",
        "score = {name : [] for name in list(classifiers)} \n",
        "for name, classifier in classifiers.items():\n",
        "  for X in data:\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, model.y,\n",
        "                                                    test_size = 0.25,\n",
        "                                                    random_state = 42)\n",
        "    classifier.fit(X_train, y_train)\n",
        "    y_pred = classifier.predict(X_test)\n",
        "    score[name].append(f1_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "xNIkFE68tMTm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Results\n",
        "indicies = list(score.keys())\n",
        "result = pd.DataFrame(score.values(), columns = ['Stemmer', 'Lemmatizer'], index = indicies)\n",
        "result.style"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "cellView": "form",
        "id": "kXBcim1b2SeU",
        "outputId": "244e2933-01d3-47a1-eb2e-5f2a908ac8ad"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f2fcdbed990>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_214fc_\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th class=\"col_heading level0 col0\" >Stemmer</th>\n",
              "      <th class=\"col_heading level0 col1\" >Lemmatizer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_214fc_level0_row0\" class=\"row_heading level0 row0\" >Logistic Regression</th>\n",
              "      <td id=\"T_214fc_row0_col0\" class=\"data row0 col0\" >0.7521</td>\n",
              "      <td id=\"T_214fc_row0_col1\" class=\"data row0 col1\" >0.7845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_214fc_level0_row1\" class=\"row_heading level0 row1\" >K-Nearest Neighbors</th>\n",
              "      <td id=\"T_214fc_row1_col0\" class=\"data row1 col0\" >0.6176</td>\n",
              "      <td id=\"T_214fc_row1_col1\" class=\"data row1 col1\" >0.5829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_214fc_level0_row2\" class=\"row_heading level0 row2\" >Support Vector Machine</th>\n",
              "      <td id=\"T_214fc_row2_col0\" class=\"data row2 col0\" >0.7382</td>\n",
              "      <td id=\"T_214fc_row2_col1\" class=\"data row2 col1\" >0.7731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_214fc_level0_row3\" class=\"row_heading level0 row3\" >Kernel SVM</th>\n",
              "      <td id=\"T_214fc_row3_col0\" class=\"data row3 col0\" >0.7299</td>\n",
              "      <td id=\"T_214fc_row3_col1\" class=\"data row3 col1\" >0.7383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_214fc_level0_row4\" class=\"row_heading level0 row4\" >Naive Bayes</th>\n",
              "      <td id=\"T_214fc_row4_col0\" class=\"data row4 col0\" >0.7266</td>\n",
              "      <td id=\"T_214fc_row4_col1\" class=\"data row4 col1\" >0.7413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_214fc_level0_row5\" class=\"row_heading level0 row5\" >Decision Tree</th>\n",
              "      <td id=\"T_214fc_row5_col0\" class=\"data row5 col0\" >0.7311</td>\n",
              "      <td id=\"T_214fc_row5_col1\" class=\"data row5 col1\" >0.7013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_214fc_level0_row6\" class=\"row_heading level0 row6\" >Random Forest</th>\n",
              "      <td id=\"T_214fc_row6_col0\" class=\"data row6 col0\" >0.7339</td>\n",
              "      <td id=\"T_214fc_row6_col1\" class=\"data row6 col1\" >0.7215</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    }
  ]
}