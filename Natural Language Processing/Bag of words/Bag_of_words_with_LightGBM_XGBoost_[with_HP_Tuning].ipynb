{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bag-of-words with LightGBM - XGBoost [with HP Tuning].ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title Installing and upgrading packages\n",
        "!pip install lightgbm --upgrade\n",
        "!pip install optuna\n",
        "!pip install xgboost --upgrade"
      ],
      "metadata": {
        "cellView": "form",
        "id": "smOKkRzoDUkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "PrY5UEKoeHeq"
      },
      "outputs": [],
      "source": [
        "#@title Importing the libraries and loading the data\n",
        "# Main libraries\n",
        "import pandas as pd\n",
        "pd.set_option(\"display.precision\", 4)\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# Text processing libraries\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Machine Learning libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# Optuna packages\n",
        "import optuna.integration.lightgbm as lgb\n",
        "from lightgbm import early_stopping\n",
        "from lightgbm import log_evaluation\n",
        "import optuna\n",
        "import xgboost as xgb\n",
        "\n",
        "# Importing the dataset\n",
        "df = pd.read_csv('reviews.tsv', delimiter='\\t',\n",
        "                 quoting = 3) # 3 is for ignoring \"\" (double quotes)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title NLP object\n",
        "class NLP(object):\n",
        "  def __init__(self, data: pd.DataFrame):\n",
        "    self.df = data\n",
        "    self.y = self.df.iloc[:, -1].values\n",
        "  \n",
        "  def create_stopwords(self) -> list:\n",
        "    \"\"\"\n",
        "    Creates English stopwords from nltk package,\n",
        "    Excludes negative words for further sentiment analysis\n",
        "    Returns a list of stopwords\n",
        "    \"\"\"\n",
        "    self.all_stopwords = stopwords.words('english')\n",
        "    negative_words = ['not', \"don't\", \"aren't\", \"didn't\", \"hadn't\", \"hasn't\", \n",
        "                  \"haven't\", \"wasn't\", \"weren't\"]\n",
        "    for word in negative_words:\n",
        "      self.all_stopwords.remove(word)\n",
        "    return self.all_stopwords\n",
        "\n",
        "  def process_review(self, index: int) -> list:\n",
        "    \"\"\"\n",
        "    Takes the index to the DataFrame,\n",
        "    first removes punctuations,\n",
        "    and second lowers the letters.\n",
        "    Returns a list including strings.\n",
        "    \"\"\"\n",
        "    review = re.sub('[^a-zA-Z]', ' ', self.df.iloc[:, 0][index])\n",
        "    review = review.lower()\n",
        "    return review.split()\n",
        "\n",
        "  def remove_special_words(self, reviews: list) -> list:\n",
        "    \"\"\"\n",
        "    Takes the stemmed or lemmatized reviews,\n",
        "    Identifies the words used once as a special word,\n",
        "    Returns the reviews without the special words.\n",
        "    \"\"\"\n",
        "    words = [word for s in reviews for word in s.split()]\n",
        "    counted = Counter(words)\n",
        "    special_words = [word for word in counted.keys() if counted[word] == 1]\n",
        "    final_reviews = reviews[:]\n",
        "    for i, sentence in enumerate(final_reviews):\n",
        "      new_sentence = []\n",
        "      for word in sentence.split():\n",
        "        if word not in special_words:\n",
        "          new_sentence.append(word)\n",
        "      final_reviews[i] = ' '.join(new_sentence)\n",
        "    return final_reviews\n",
        "\n",
        "  def apply_stemmer(self) -> list:\n",
        "    \"\"\"\n",
        "    Fetches the reviews from the DataFrame,\n",
        "    (I) process the individual review via process_review(),\n",
        "    (II) removes the stopwords taken from create_stopwords(),\n",
        "    (III) removes the conjuctions of verbs (e.g. loved -> love) via Stemmer,\n",
        "    (IV) removes the special_words via remove_special_words().\n",
        "    Returns a list for Bag-of-words model\n",
        "    \"\"\"\n",
        "    corpus, stop_words = [], self.create_stopwords()\n",
        "    for i in range(self.df.shape[0]):\n",
        "      # Step I\n",
        "      review = self.process_review(i)\n",
        "      # Step II and III\n",
        "      stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
        "      review = [stemmer.stem(word) for word in review\n",
        "              if not word in set(stop_words)]\n",
        "      corpus.append(' '.join(review))\n",
        "\n",
        "    # Step IV\n",
        "    review = self.remove_special_words(corpus)\n",
        "    return corpus\n",
        "  \n",
        "  def apply_lemmatizer(self) -> list:\n",
        "    \"\"\"\n",
        "    Fetches the reviews from the DataFrame,\n",
        "    (I) process the individual review via process_review(),\n",
        "    (II) removes the stopwords taken from create_stopwords(),\n",
        "    (III) removes the conjuctions of verbs (e.g. loved -> love) via Lemmatizer,\n",
        "    (IV) removes the special_words via remove_special_words().\n",
        "    Returns a list for Bag-of-words model\n",
        "    \"\"\"\n",
        "    corpus, stop_words = [], self.create_stopwords()\n",
        "    for i in range(self.df.shape[0]):\n",
        "      # Step I\n",
        "      review = self.process_review(i)\n",
        "      # Step II and III\n",
        "      lemmatizer = WordNetLemmatizer()\n",
        "      review = [lemmatizer.lemmatize(word, pos = 'v') for word in review\n",
        "              if not word in set(stop_words)]\n",
        "      corpus.append(' '.join(review))\n",
        "    \n",
        "    # Step IV\n",
        "    review = self.remove_special_words(corpus)\n",
        "    return corpus\n",
        "\n",
        "  def bow(self) -> dict:\n",
        "    \"\"\"\n",
        "    Fetches the data and applies first stemmer and then lemmatizer\n",
        "    to build the bag-of-word vector.\n",
        "    Returns a dict including vectorized data for the stemmer and lemmatizer.\n",
        "    \"\"\"\n",
        "    reviews_stemmer = self.apply_stemmer()\n",
        "    reviews_lemmatizer = self.apply_lemmatizer()\n",
        "    dataset = {'Stemmer': reviews_stemmer, 'Lemmatizer' : reviews_lemmatizer}\n",
        "    for name, r in dataset.items():\n",
        "      cv = CountVectorizer()\n",
        "      dataset[name] = cv.fit_transform(r).toarray()\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "XH497sNxfDHt",
        "cellView": "form"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Optuna Lightgbm tuner\n",
        "def tuner(data, target):\n",
        "  # Splitting the data\n",
        "  X_train, X_test, y_train, y_test = train_test_split(data, target,\n",
        "                                                    test_size = 0.25,\n",
        "                                                    random_state = 42)\n",
        "  \n",
        "  # Setting up the data for the model\n",
        "  dtrain = lgb.Dataset(X_train, label = y_train)\n",
        "  dtest = lgb.Dataset(X_test, label = y_test)\n",
        "\n",
        "  # Parameters for classification\n",
        "  params = {\n",
        "      'objective' : 'binary',\n",
        "      'metric' : 'binary_logloss',\n",
        "      'verbosity' : -1,\n",
        "      'boosting_type' : 'gbdt'\n",
        "  }\n",
        "\n",
        "  # Training the model\n",
        "  model = lgb.train(\n",
        "      params,\n",
        "      dtrain,\n",
        "      valid_sets = [dtrain, dtest],\n",
        "      callbacks = [early_stopping(100), log_evaluation(100)]\n",
        "  )\n",
        "\n",
        "  # Results\n",
        "  y_pred = np.rint(model.predict(X_test, \n",
        "                                     num_iteration = model.best_iteration))\n",
        "  score = f1_score(y_test, y_pred)\n",
        "  return score"
      ],
      "metadata": {
        "cellView": "form",
        "id": "y6b24yvjC4Il"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Optuna XGBoost tuner\n",
        "\n",
        "def objective(trial):\n",
        "    \n",
        "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "    dvalid = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "    param = {\n",
        "        \"verbosity\": 0,\n",
        "        \"objective\": \"binary:logistic\",\n",
        "        \"eval_metric\": \"auc\",\n",
        "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n",
        "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
        "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
        "    }\n",
        "\n",
        "    if param[\"booster\"] == \"gbtree\" or param[\"booster\"] == \"dart\":\n",
        "        param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 1, 9)\n",
        "        param[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n",
        "        param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
        "        param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
        "    if param[\"booster\"] == \"dart\":\n",
        "        param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
        "        param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
        "        param[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n",
        "        param[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n",
        "\n",
        "    # Add a callback for pruning.\n",
        "    pruning_callback = optuna.integration.XGBoostPruningCallback(trial, \"validation-auc\")\n",
        "    model = xgb.train(param, dtrain, evals=[(dvalid, \"validation\")], callbacks=[pruning_callback])\n",
        "\n",
        "    y_pred = np.rint(model.predict(dvalid))\n",
        "    score = f1_score(y_test, y_pred)\n",
        "    return score"
      ],
      "metadata": {
        "cellView": "form",
        "id": "yFGnLp1aFEuk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Training the LightGBM\n",
        "model = NLP(df)\n",
        "dataset = model.bow()\n",
        "lgbm_results = {}\n",
        "for name, X in dataset.items():\n",
        "  lgbm_results[name] = tuner(X, model.y)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "YcBNXQOhD3mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Training the XGBoost\n",
        "xgb_results = {}\n",
        "for name, X in dataset.items():\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, model.y, \n",
        "                                                          test_size=0.25, \n",
        "                                                          random_state = 42)\n",
        "  # Find the best parameters with Optuna\n",
        "  study = optuna.create_study(\n",
        "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction=\"maximize\"\n",
        "    )\n",
        "  study.optimize(objective, n_trials=100)\n",
        "  best_params =  study.best_trial.params\n",
        "\n",
        "  # Train the XGBClassifier with the best params\n",
        "  classifier = xgb.XGBClassifier(objective = \"binary:logistic\", eval_metric = \"auc\",\n",
        "                                 use_label_encoder=False ,**best_params)\n",
        "  classifier.fit(X_train, y_train)\n",
        "  y_pred = np.rint(classifier.predict(X_test))\n",
        "  xgb_results[name] = f1_score(y_test, y_pred)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "RuHbpgEyHJvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Results\n",
        "scores = np.concatenate((np.array([list(lgbm_results.values())]), \n",
        "                         np.array([list(xgb_results.values())])))\n",
        "results = pd.DataFrame(scores, columns = list(lgbm_results.keys()),\n",
        "                       index = ['LightGBM', 'XGBoost'])\n",
        "results.style"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "cellView": "form",
        "id": "DfxKs2DNPJMO",
        "outputId": "1d1d291b-40bd-4c56-ee61-8fc8936abb69"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f04d0f9c490>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_317b0_\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th class=\"col_heading level0 col0\" >Stemmer</th>\n",
              "      <th class=\"col_heading level0 col1\" >Lemmatizer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_317b0_level0_row0\" class=\"row_heading level0 row0\" >LightGBM</th>\n",
              "      <td id=\"T_317b0_row0_col0\" class=\"data row0 col0\" >0.6762</td>\n",
              "      <td id=\"T_317b0_row0_col1\" class=\"data row0 col1\" >0.6827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_317b0_level0_row1\" class=\"row_heading level0 row1\" >XGBoost</th>\n",
              "      <td id=\"T_317b0_row1_col0\" class=\"data row1 col0\" >0.7706</td>\n",
              "      <td id=\"T_317b0_row1_col1\" class=\"data row1 col1\" >0.7897</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}